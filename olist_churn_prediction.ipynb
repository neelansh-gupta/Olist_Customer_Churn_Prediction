{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Olist Customer Churn Prediction\n",
    "**Author:** Data Science Team  \n",
    "**Date:** September 2025  \n",
    "**Python Version:** 3.9+  \n",
    "\n",
    "## Objective\n",
    "Engineer behavioral features per customer and predict a binary churn label where a customer is considered churned if there has been no purchase in the last 180 days.\n",
    "\n",
    "## Dataset\n",
    "Olist Brazilian E-Commerce Public Dataset: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=== OLIST CUSTOMER CHURN PREDICTION ===\")\n",
    "print(f\"Random State: {RANDOM_STATE}\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. LOADING DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the required datasets\n",
    "    customers_df = pd.read_csv('olist_customers_dataset.csv')\n",
    "    orders_df = pd.read_csv('olist_orders_dataset.csv')\n",
    "    payments_df = pd.read_csv('olist_order_payments_dataset.csv')\n",
    "    \n",
    "    print(f\"‚úì Customers data loaded: {customers_df.shape}\")\n",
    "    print(f\"‚úì Orders data loaded: {orders_df.shape}\")\n",
    "    print(f\"‚úì Payments data loaded: {payments_df.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚úó Error loading files: {e}\")\n",
    "    print(\"Please ensure the following CSV files are in the current directory:\")\n",
    "    print(\"- olist_customers_dataset.csv\")\n",
    "    print(\"- olist_orders_dataset.csv\") \n",
    "    print(\"- olist_order_payments_dataset.csv\")\n",
    "    print(\"\\nDownload from: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "print(\"\\nDATA OVERVIEW:\")\n",
    "print(\"Customers columns:\", list(customers_df.columns))\n",
    "print(\"Orders columns:\", list(orders_df.columns))\n",
    "print(\"Payments columns:\", list(payments_df.columns))\n",
    "\n",
    "# Check for required columns\n",
    "required_customer_cols = ['customer_id', 'customer_unique_id', 'customer_city', 'customer_state']\n",
    "required_order_cols = ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp']\n",
    "required_payment_cols = ['order_id', 'payment_value']\n",
    "\n",
    "print(f\"\\n‚úì Customer required columns present: {all(col in customers_df.columns for col in required_customer_cols)}\")\n",
    "print(f\"‚úì Order required columns present: {all(col in orders_df.columns for col in required_order_cols)}\")\n",
    "print(f\"‚úì Payment required columns present: {all(col in payments_df.columns for col in required_payment_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep-header",
   "metadata": {},
   "source": [
    "## 2. Data Understanding and Customer Granularity Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "granularity-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. DATA UNDERSTANDING AND CUSTOMER GRANULARITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze customer granularity choice\n",
    "print(\"CUSTOMER GRANULARITY ANALYSIS:\")\n",
    "print(f\"Unique customer_id count: {customers_df['customer_id'].nunique():,}\")\n",
    "print(f\"Unique customer_unique_id count: {customers_df['customer_unique_id'].nunique():,}\")\n",
    "print(f\"Total customer records: {len(customers_df):,}\")\n",
    "\n",
    "# Check for one-to-many relationships\n",
    "customer_id_to_unique = customers_df.groupby('customer_id')['customer_unique_id'].nunique()\n",
    "multiple_unique_ids = customer_id_to_unique[customer_id_to_unique > 1]\n",
    "print(f\"Customer IDs with multiple unique IDs: {len(multiple_unique_ids)}\")\n",
    "\n",
    "unique_to_customer_id = customers_df.groupby('customer_unique_id')['customer_id'].nunique()\n",
    "multiple_customer_ids = unique_to_customer_id[unique_to_customer_id > 1]\n",
    "print(f\"Unique customer IDs with multiple customer_ids: {len(multiple_customer_ids)}\")\n",
    "\n",
    "# Decision: Use customer_unique_id as the primary key\n",
    "CUSTOMER_KEY = 'customer_unique_id'\n",
    "print(f\"\\nüìã DECISION: Using '{CUSTOMER_KEY}' as customer granularity\")\n",
    "print(\"JUSTIFICATION: customer_unique_id represents the true unique customer,\")\n",
    "print(\"while customer_id can have multiple entries per unique customer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Reference Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. DATA PREPROCESSING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Convert timestamp and filter delivered orders\n",
    "orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])\n",
    "delivered_orders = orders_df[orders_df['order_status'] == 'delivered'].copy()\n",
    "\n",
    "print(f\"ORDER STATUS FILTERING:\")\n",
    "print(f\"Total orders: {len(orders_df):,}\")\n",
    "print(f\"Delivered orders: {len(delivered_orders):,}\")\n",
    "print(f\"Delivered orders percentage: {len(delivered_orders)/len(orders_df)*100:.1f}%\")\n",
    "\n",
    "# Define reference date as max order_purchase_timestamp in delivered orders\n",
    "reference_date = delivered_orders['order_purchase_timestamp'].max()\n",
    "print(f\"\\nREFERENCE DATE: {reference_date}\")\n",
    "print(f\"This represents the latest delivered order in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-eng-header",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: RFM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. FEATURE ENGINEERING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 1: Aggregate payments per order (sum multiple payment rows)\n",
    "print(\"Aggregating payments per order...\")\n",
    "order_payments = payments_df.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "print(f\"Orders with payment data: {len(order_payments):,}\")\n",
    "\n",
    "# Step 2: Join delivered orders with payments\n",
    "delivered_orders_with_payments = delivered_orders.merge(order_payments, on='order_id', how='left')\n",
    "print(f\"Delivered orders with payment data: {len(delivered_orders_with_payments):,}\")\n",
    "\n",
    "# Handle missing payment values\n",
    "missing_payments = delivered_orders_with_payments['payment_value'].isna().sum()\n",
    "if missing_payments > 0:\n",
    "    print(f\"‚ö†Ô∏è Orders with missing payment data: {missing_payments}\")\n",
    "    delivered_orders_with_payments['payment_value'].fillna(0, inplace=True)\n",
    "    print(\"Filled missing payments with 0\")\n",
    "\n",
    "# Step 3: Join with customers to get customer_unique_id\n",
    "orders_customers = delivered_orders_with_payments.merge(\n",
    "    customers_df[['customer_id', CUSTOMER_KEY]], \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "print(f\"Orders joined with customer unique IDs: {len(orders_customers):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfm-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate RFM features per customer\n",
    "print(\"\\nCalculating RFM features...\")\n",
    "\n",
    "# Recency: days since last purchase\n",
    "customer_last_purchase = orders_customers.groupby(CUSTOMER_KEY)['order_purchase_timestamp'].max().reset_index()\n",
    "customer_last_purchase['recency'] = (reference_date - customer_last_purchase['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# Frequency: number of delivered orders\n",
    "customer_frequency = orders_customers.groupby(CUSTOMER_KEY).size().reset_index(name='frequency')\n",
    "\n",
    "# Monetary: total spend\n",
    "customer_monetary = orders_customers.groupby(CUSTOMER_KEY)['payment_value'].sum().reset_index()\n",
    "customer_monetary.rename(columns={'payment_value': 'monetary_value'}, inplace=True)\n",
    "\n",
    "# Combine all features\n",
    "rfm_features = customer_last_purchase[[CUSTOMER_KEY, 'recency']].merge(\n",
    "    customer_frequency, on=CUSTOMER_KEY\n",
    ").merge(\n",
    "    customer_monetary, on=CUSTOMER_KEY\n",
    ")\n",
    "\n",
    "print(f\"RFM features calculated for {len(rfm_features):,} customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create churn target (180-day threshold as specified)\n",
    "CHURN_THRESHOLD = 180\n",
    "rfm_features['churn'] = (rfm_features['recency'] > CHURN_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\nCHURN TARGET CREATION:\")\n",
    "print(f\"Churn threshold: {CHURN_THRESHOLD} days\")\n",
    "print(f\"Customers with recency > {CHURN_THRESHOLD} days are labeled as churned (1)\")\n",
    "\n",
    "print(\"\\nFEATURE SUMMARY:\")\n",
    "print(f\"Recency - Mean: {rfm_features['recency'].mean():.1f} days, Median: {rfm_features['recency'].median():.1f} days\")\n",
    "print(f\"Frequency - Mean: {rfm_features['frequency'].mean():.2f}, Median: {rfm_features['frequency'].median():.2f}\")\n",
    "print(f\"Monetary - Mean: R${rfm_features['monetary_value'].mean():.2f}, Median: R${rfm_features['monetary_value'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target-analysis-header",
   "metadata": {},
   "source": [
    "## 5. Target Analysis and Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n5. TARGET ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "churn_distribution = rfm_features['churn'].value_counts()\n",
    "churn_percentage = rfm_features['churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"CHURN DISTRIBUTION:\")\n",
    "print(f\"Not Churned (0): {churn_distribution[0]:,} ({churn_percentage[0]:.1f}%)\")\n",
    "print(f\"Churned (1): {churn_distribution[1]:,} ({churn_percentage[1]:.1f}%)\")\n",
    "\n",
    "class_imbalance_ratio = churn_distribution[0] / churn_distribution[1]\n",
    "print(f\"Class imbalance ratio: {class_imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if class_imbalance_ratio > 2 or class_imbalance_ratio < 0.5:\n",
    "    print(\"‚ö†Ô∏è Class imbalance detected - will monitor model performance carefully\")\n",
    "else:\n",
    "    print(\"‚úì Classes are reasonably balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-header",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Recency distribution by churn\n",
    "axes[0, 0].hist(rfm_features[rfm_features['churn']==0]['recency'], \n",
    "                alpha=0.7, label='Not Churned', bins=50, color='blue')\n",
    "axes[0, 0].hist(rfm_features[rfm_features['churn']==1]['recency'], \n",
    "                alpha=0.7, label='Churned', bins=50, color='red')\n",
    "axes[0, 0].axvline(x=CHURN_THRESHOLD, color='black', linestyle='--', \n",
    "                   alpha=0.8, label=f'{CHURN_THRESHOLD}-day threshold')\n",
    "axes[0, 0].set_xlabel('Recency (days)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Recency Distribution by Churn Status')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Frequency distribution by churn\n",
    "axes[0, 1].hist(rfm_features[rfm_features['churn']==0]['frequency'], \n",
    "                alpha=0.7, label='Not Churned', bins=30, color='blue')\n",
    "axes[0, 1].hist(rfm_features[rfm_features['churn']==1]['frequency'], \n",
    "                alpha=0.7, label='Churned', bins=30, color='red')\n",
    "axes[0, 1].set_xlabel('Frequency (# orders)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Order Frequency by Churn Status')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Monetary distribution by churn (log scale due to skewness)\n",
    "monetary_not_churned = rfm_features[rfm_features['churn']==0]['monetary_value']\n",
    "monetary_churned = rfm_features[rfm_features['churn']==1]['monetary_value']\n",
    "axes[1, 0].hist(np.log1p(monetary_not_churned), alpha=0.7, label='Not Churned', \n",
    "                bins=50, color='blue')\n",
    "axes[1, 0].hist(np.log1p(monetary_churned), alpha=0.7, label='Churned', \n",
    "                bins=50, color='red')\n",
    "axes[1, 0].set_xlabel('Log(Monetary Value + 1)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Monetary Value Distribution by Churn Status')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Correlation heatmap\n",
    "correlation_matrix = rfm_features[['recency', 'frequency', 'monetary_value', 'churn']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfm_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualizations saved as 'rfm_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling-header",
   "metadata": {},
   "source": [
    "## 7. Model Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n7. MODEL PREPARATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['recency', 'frequency', 'monetary_value']\n",
    "X = rfm_features[feature_columns].copy()\n",
    "y = rfm_features['churn'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"X missing values: {X.isnull().sum().sum()}\")\n",
    "print(f\"y missing values: {y.isnull().sum()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_counts = np.isinf(X).sum().sum()\n",
    "if inf_counts > 0:\n",
    "    print(f\"‚ö†Ô∏è Infinite values detected: {inf_counts}\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"Infinite values replaced with median\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nChurn distribution in train set: {y_train.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Churn distribution in test set: {y_test.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "\n",
    "# Feature scaling for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"‚úì Features scaled for logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n8. MODEL TRAINING AND EVALUATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize models as required\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, max_depth=10)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_folds = 5\n",
    "cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Training and evaluating models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Select appropriate training data\n",
    "    if name == 'Logistic Regression':\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='f1')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_model, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'cv_f1_mean': cv_scores.mean(),\n",
    "        'cv_f1_std': cv_scores.std(),\n",
    "        'test_accuracy': accuracy,\n",
    "        'test_precision': precision,\n",
    "        'test_recall': recall,\n",
    "        'test_f1': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"   CV F1-Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Test Precision: {precision:.4f}\")\n",
    "    print(f\"   Test Recall: {recall:.4f}\")\n",
    "    print(f\"   Test F1-Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Results Comparison and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n9. RESULTS COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'CV F1 Mean': f\"{data['cv_f1_mean']:.4f}\",\n",
    "        'CV F1 Std': f\"¬±{data['cv_f1_std']:.4f}\",\n",
    "        'Test Accuracy': f\"{data['test_accuracy']:.4f}\",\n",
    "        'Test Precision': f\"{data['test_precision']:.4f}\",\n",
    "        'Test Recall': f\"{data['test_recall']:.4f}\",\n",
    "        'Test F1': f\"{data['test_f1']:.4f}\"\n",
    "    }\n",
    "    for name, data in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"MODEL PERFORMANCE COMPARISON:\")\n",
    "print(results_df)\n",
    "\n",
    "# Identify best model based on F1-Score\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['test_f1'])\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name} (F1-Score: {results[best_model_name]['test_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-header",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n10. CONFUSION MATRIX - BEST MODEL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get predictions from best model\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Add performance metrics to the plot\n",
    "metrics_text = f\"\"\"\n",
    "Test Metrics:\n",
    "Accuracy: {results[best_model_name]['test_accuracy']:.4f}\n",
    "Precision: {results[best_model_name]['test_precision']:.4f}\n",
    "Recall: {results[best_model_name]['test_recall']:.4f}\n",
    "F1-Score: {results[best_model_name]['test_f1']:.4f}\n",
    "\"\"\"\n",
    "plt.text(2.1, 0.5, metrics_text, fontsize=12, verticalalignment='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Confusion matrix saved as 'confusion_matrix_best_model.png'\")\n",
    "\n",
    "# Detailed confusion matrix analysis\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nCONFUSION MATRIX BREAKDOWN:\")\n",
    "print(f\"True Negatives (TN): {tn:,}\")\n",
    "print(f\"False Positives (FP): {fp:,}\")\n",
    "print(f\"False Negatives (FN): {fn:,}\")\n",
    "print(f\"True Positives (TP): {tp:,}\")\n",
    "print(f\"\\nSpecificity (True Negative Rate): {tn/(tn+fp):.4f}\")\n",
    "print(f\"Sensitivity (True Positive Rate): {tp/(tp+fn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-header",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for Random Forest\n",
    "if 'Random Forest' in trained_models:\n",
    "    print(\"\\n11. FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    rf_model = trained_models['Random Forest']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"FEATURE IMPORTANCE RANKING (Random Forest):\")\n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Feature importance plot saved as 'feature_importance.png'\")\n",
    "else:\n",
    "    print(\"\\nFeature importance analysis skipped (Random Forest not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights-header",
   "metadata": {},
   "source": [
    "## 11. Business Insights and Customer Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "business-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n12. BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(f\"‚Ä¢ Dataset contains {len(rfm_features):,} unique customers\")\n",
    "print(f\"‚Ä¢ Churn rate: {churn_percentage[1]:.1f}% ({churn_distribution[1]:,} churned customers)\")\n",
    "print(f\"‚Ä¢ Best performing model: {best_model_name} (F1: {results[best_model_name]['test_f1']:.4f})\")\n",
    "\n",
    "# Analyze churned vs non-churned customer characteristics\n",
    "churned_customers = rfm_features[rfm_features['churn'] == 1]\n",
    "active_customers = rfm_features[rfm_features['churn'] == 0]\n",
    "\n",
    "print(f\"\\nCUSTOMER SEGMENT COMPARISON:\")\n",
    "print(f\"Average recency - Churned: {churned_customers['recency'].mean():.1f} days vs Active: {active_customers['recency'].mean():.1f} days\")\n",
    "print(f\"Average frequency - Churned: {churned_customers['frequency'].mean():.2f} vs Active: {active_customers['frequency'].mean():.2f}\")\n",
    "print(f\"Average monetary - Churned: R${churned_customers['monetary_value'].mean():.2f} vs Active: R${active_customers['monetary_value'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nBUSINESS RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Focus retention efforts on customers approaching 180-day mark\")\n",
    "print(\"‚Ä¢ Implement re-engagement campaigns for high-value customers showing early churn signals\")\n",
    "print(\"‚Ä¢ Monitor customer purchase frequency as an early warning indicator\")\n",
    "print(\"‚Ä¢ Consider personalized offers based on historical monetary value\")\n",
    "print(\"‚Ä¢ Develop targeted campaigns for different customer segments based on RFM scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limitations-header",
   "metadata": {},
   "source": [
    "## 12. Model Limitations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limitations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n13. LIMITATIONS AND NEXT STEPS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"CURRENT LIMITATIONS:\")\n",
    "print(\"‚Ä¢ Fixed 180-day churn definition may not suit all customer segments or business contexts\")\n",
    "print(\"‚Ä¢ Limited to RFM features - missing behavioral, demographic, and product preference data\")\n",
    "print(\"‚Ä¢ No seasonal or temporal trend analysis\")\n",
    "print(f\"‚Ä¢ Class balance: {churn_percentage[0]:.1f}%/{churn_percentage[1]:.1f}% split\")\n",
    "print(\"‚Ä¢ Model assumes static customer behavior patterns\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS FOR MODEL IMPROVEMENT:\")\n",
    "print(\"‚Ä¢ Experiment with different churn definitions (90, 120, 270 days)\")\n",
    "print(\"‚Ä¢ Implement time-series cross-validation to account for temporal patterns\")\n",
    "print(\"‚Ä¢ Add feature engineering: purchase seasonality, product categories, customer lifetime\")\n",
    "print(\"‚Ä¢ Consider ensemble methods or advanced algorithms (XGBoost, Neural Networks)\")\n",
    "print(\"‚Ä¢ Implement threshold optimization for different business cost scenarios\")\n",
    "print(\"‚Ä¢ Deploy model for real-time scoring and A/B testing of retention strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumptions-header",
   "metadata": {},
   "source": [
    "## 13. Key Assumptions and Design Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumptions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n14. KEY ASSUMPTIONS AND DESIGN DECISIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"DATA ASSUMPTIONS:\")\n",
    "print(f\"‚Ä¢ Customer granularity: {CUSTOMER_KEY} represents unique customers\")\n",
    "print(\"‚Ä¢ Only 'delivered' orders are considered completed purchases\")\n",
    "print(\"‚Ä¢ Multiple payment rows per order are summed to get total order value\")\n",
    "print(\"‚Ä¢ Missing payment values are filled with 0 (assumed free orders or data issues)\")\n",
    "print(f\"‚Ä¢ Reference date: {reference_date.strftime('%Y-%m-%d')} (latest delivered order)\")\n",
    "\n",
    "print(\"\\nMODELING ASSUMPTIONS:\")\n",
    "print(f\"‚Ä¢ Churn threshold: {CHURN_THRESHOLD} days (business requirement)\")\n",
    "print(\"‚Ä¢ Features are independent and stationary over time\")\n",
    "print(\"‚Ä¢ Customer behavior patterns are consistent across segments\")\n",
    "print(\"‚Ä¢ Linear relationship assumption for Logistic Regression\")\n",
    "print(\"‚Ä¢ Tree-based patterns assumption for Random Forest\")\n",
    "\n",
    "print(\"\\nEDGE CASE HANDLING:\")\n",
    "print(\"‚Ä¢ Customers with only non-delivered orders: excluded from analysis\")\n",
    "print(\"‚Ä¢ Orders with missing payments: payment_value = 0\")\n",
    "print(\"‚Ä¢ Feature scaling applied to Logistic Regression, not Random Forest\")\n",
    "print(\"‚Ä¢ Stratified sampling maintains class distribution in train/test split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reproducibility-header",
   "metadata": {},
   "source": [
    "## 14. Reproducibility Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reproducibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n15. REPRODUCIBILITY INFORMATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"ENVIRONMENT DETAILS:\")\n",
    "print(f\"‚Ä¢ Python: 3.9+\")\n",
    "print(f\"‚Ä¢ pandas: {pd.__version__}\")\n",
    "print(f\"‚Ä¢ numpy: {np.__version__}\")\n",
    "print(f\"‚Ä¢ scikit-learn: Available\")\n",
    "print(f\"‚Ä¢ matplotlib: Available\")\n",
    "print(f\"‚Ä¢ seaborn: Available\")\n",
    "\n",
    "print(f\"\\nREPRODUCIBILITY SETTINGS:\")\n",
    "print(f\"‚Ä¢ Random state: {RANDOM_STATE}\")\n",
    "print(f\"‚Ä¢ Train/test split: 80/20 with stratification\")\n",
    "print(f\"‚Ä¢ Cross-validation: {cv_folds}-fold stratified\")\n",
    "print(f\"‚Ä¢ Churn threshold: {CHURN_THRESHOLD} days\")\n",
    "print(f\"‚Ä¢ Customer key: {CUSTOMER_KEY}\")\n",
    "\n",
    "print(f\"\\nDATASET REQUIREMENTS:\")\n",
    "print(\"‚Ä¢ olist_customers_dataset.csv\")\n",
    "print(\"‚Ä¢ olist_orders_dataset.csv\")\n",
    "print(\"‚Ä¢ olist_order_payments_dataset.csv\")\n",
    "print(\"‚Ä¢ Download from: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\")\n",
    "\n",
    "print(f\"\\nGENERATED OUTPUTS:\")\n",
    "print(\"‚Ä¢ rfm_analysis.png - EDA visualizations\")\n",
    "print(\"‚Ä¢ confusion_matrix_best_model.png - Best model performance\")\n",
    "if 'Random Forest' in trained_models:\n",
    "    print(\"‚Ä¢ feature_importance.png - Random Forest feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 15. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"‚Ä¢ Analyzed {len(rfm_features):,} unique customers from Olist dataset\")\n",
    "print(f\"‚Ä¢ Engineered RFM features with {CHURN_THRESHOLD}-day churn definition\")\n",
    "print(f\"‚Ä¢ Compared Logistic Regression vs Random Forest models\")\n",
    "print(f\"‚Ä¢ Best model: {best_model_name} with F1-Score of {results[best_model_name]['test_f1']:.4f}\")\n",
    "print(f\"‚Ä¢ Churn rate: {churn_percentage[1]:.1f}% of customers\")\n",
    "print(f\"\\nThis analysis provides a baseline for customer churn prediction\")\n",
    "print(f\"and identifies key areas for business intervention and model improvement.\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
